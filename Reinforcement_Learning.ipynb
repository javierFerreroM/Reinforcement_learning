{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMHtrtU0mshSRfno/m3CgVC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/javierFerreroM/Reinforcement_learning/blob/main/Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Car racing problem"
      ],
      "metadata": {
        "id": "RvL5esIEr5_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bibliografy\n",
        "https://github.com/andywu0913/OpenAI-GYM-CarRacing-DQN;\n",
        "https://github.com/wpiszlogin/driver_critic/blob/main/base_solution.py; \n",
        "https://www.gymlibrary.ml/environments/box2d/car_racing/"
      ],
      "metadata": {
        "id": "rNFlKhXZGgGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize and install the dependencies"
      ],
      "metadata": {
        "id": "tlapvPz4r_Zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/car_racing.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHN8yfbAa34N",
        "outputId": "47d17316-824b-41a1-9649-c29c0c8b8b4b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/car_racing.py\", line 13, in <module>\n",
            "    from gym.utils.renderer import Renderer\n",
            "ModuleNotFoundError: No module named 'gym.utils.renderer'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFSScvmEQ5zh",
        "outputId": "ba697205-6ead-4725-b078-f07a8ee93f9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.21.6)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.4.1)\n",
            "Collecting box2d-py~=2.3.5\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 14.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]) (0.16.0)\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from docutils.nodes import topic\n",
        "#from tools import *\n"
      ],
      "metadata": {
        "id": "al1yhBkFPhMl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Description\n",
        "The easiest control task to learn from pixels - a top-down racing environment. The generated track is random every episode.\n",
        "\n",
        "Some indicators are shown at the bottom of the window along with the state RGB buffer. From left to right: true speed, four ABS sensors, steering wheel position, and gyroscope."
      ],
      "metadata": {
        "id": "IOL1xkH5Qqdp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fjkd8e9M6fdH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcca4ab8-7dad-4791-a234-c91219087ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of State Space ->  96\n",
            "Size of Action Space ->  3\n",
            "Actions are ->  Box(-1.0, 1.0, (3,), float32)\n",
            "Max Value of Action ->  1.0\n",
            "Min Value of Action ->  -1.0\n",
            "Max Value of Observation ->  255\n",
            "Min Value of Observation ->  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        }
      ],
      "source": [
        "# We use version 0 as the original version\n",
        "# This cell describes the characteristics of the problem\n",
        "problem = \"CarRacing-v0\"\n",
        "env = gym.make(problem)\n",
        "\n",
        "num_states = env.observation_space.shape[0]\n",
        "\n",
        "print(\"Size of State Space ->  {}\".format(num_states))\n",
        "num_actions = env.action_space.shape[0]\n",
        "action_space = env.action_space\n",
        "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
        "\n",
        "print(\"Actions are ->  {}\".format(action_space))\n",
        "\n",
        "upper_bound = env.action_space.high[0]\n",
        "lower_bound = env.action_space.low[0]\n",
        "\n",
        "observation_upper_bound = env.observation_space.high[0][0][0]\n",
        "observation_lower_bound = env.observation_space.low[0][0][0]\n",
        "\n",
        "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
        "print(\"Min Value of Action ->  {}\".format(lower_bound))\n",
        "\n",
        "print(\"Max Value of Observation ->  {}\".format(observation_upper_bound))\n",
        "print(\"Min Value of Observation ->  {}\".format(observation_lower_bound))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Action space\n",
        "As observed, there are three actions:\n",
        "\n",
        "*   Turn (-1 left and +1 is right)\n",
        "*   Accelerate\n",
        "*   Breaking\n",
        "\n",
        "##Observation space\n",
        "The observation space (the image) is 96x96 pixels\n",
        "\n",
        "\n",
        "##Rewards\n",
        "The reward is -0.1 every frme and +1000/N for every track visited, where N is the total number of tiles visited in the track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points.\n",
        "\n",
        "If the car goes outside the playfield, it receives -100 reward and dies."
      ],
      "metadata": {
        "id": "WhT0Nc12TiLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Actor-critic and policy definition\n",
        "Actor-critic learning is a reinforcement-learning technique in which you simultaneously learn a policy function and a value function. The policy function tells you how to make decisions, and the value function helps improve the training process for the value function.\n",
        "\n",
        "The actor takes as input the state and outputs the best action. It essentially controls how the agent behaves by learning the optimal policy (policy-based). The critic, on the other hand, evaluates the action by computing the value function (value based). Those two models participate in a game where they both get better in their own role as the time passes. The result is that the overall architecture will learn to play the game more efficiently than the two methods separately.\n",
        "\n",
        "In the following two boxes we define the actor and the critic. "
      ],
      "metadata": {
        "id": "yzuR-Dr3dox9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actor"
      ],
      "metadata": {
        "id": "VkzPAtIksk86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We build here the behaviour of the actor\n",
        "def build_actor(state_shape, name=\"Actor\"):\n",
        "      inputs = layers.Input(shape=state_shape)\n",
        "      x = inputs\n",
        "      x = layers.Conv2D(16, kernel_size=(5, 5), strides=(4, 4), padding='valid', use_bias=False, activation=\"relu\")(x)\n",
        "      x = layers.Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='valid', use_bias=False, activation=\"relu\")(x)\n",
        "      x = layers.Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='valid', use_bias=False, activation=\"relu\")(x)\n",
        "\n",
        "      x = layers.Flatten()(x)\n",
        "      x = layers.Dense(64, activation='relu')(x)\n",
        "      last_init = tf.random_uniform_initializer(minval=-0.005, maxval=0.005)\n",
        "      y = layers.Dense(num_actions, activation='tanh')(x)\n",
        "\n",
        "      model = Model(inputs=inputs, outputs=y, name=name)\n",
        "      model.summary()\n",
        "      return model"
      ],
      "metadata": {
        "id": "loQChPcqYLDE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Critic\n",
        "The value function is the critic: it tracks whether the agent is ahead or behind in the course of the game. That feedback guides the training process, in the same way that a game review can guide your own study.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8znxfe2Sdhxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We build here the behaviour of the Critic.\n",
        "def build_critic(state_shape, name=\"Critic\"):\n",
        "    state_inputs = layers.Input(shape=state_shape)\n",
        "    x = state_inputs\n",
        "    x = layers.Conv2D(16, kernel_size=(5, 5), strides=(4, 4), padding='valid', use_bias=False, activation=\"relu\")(x)\n",
        "    x = layers.Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='valid', use_bias=False, activation=\"relu\")(x)\n",
        "    x = layers.Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='valid', use_bias=False, activation=\"relu\")(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    action_inputs = layers.Input(shape=(self.model_action_out,))\n",
        "    x = layers.concatenate([x, action_inputs])\n",
        "\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dense(32, activation='relu')(x)\n",
        "    y = layers.Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=[state_inputs, action_inputs], outputs=y, name=name)\n",
        "    model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "qBZgj3f4dJwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Policy\n",
        "Now we define the policy. The policy is a mapping that selects actions based on the observations from the environment. Typically, the policy is a function approximator with tunable parameters, such as a deep neural network."
      ],
      "metadata": {
        "id": "JgUCoG8pfVQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy will allow us to get the actions during training\n",
        "def policy(state, actor, noise):\n",
        "        prep_state = preprocess_state(state)\n",
        "        if actor is None:\n",
        "            init_networks(prep_state.shape)\n",
        "\n",
        "        # Get result from a network\n",
        "        tensor_state = tf.expand_dims(tf.convert_to_tensor(prep_state), 0)\n",
        "        actor_output = actor(tensor_state).numpy()\n",
        "\n",
        "        # Add noise\n",
        "        actor_output = actor_output[0] + noise.generate()\n",
        "        env_action = actor_output\n",
        "\n",
        "        # Clip min-max\n",
        "        env_action = np.clip(np.array(env_action), a_min= action_space.low, a_max= action_space.high)\n",
        "        return env_action, actor_output"
      ],
      "metadata": {
        "id": "nQF_RPr_iLaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We initialize the networks for the first step\n",
        "def init_networks(self, state_shape):\n",
        "  # Networks initialization\n",
        "  actor  = build_actor(state_shape)\n",
        "  critic = build_critic(state_shape)\n",
        "\n",
        "  # Build target networks in the same way\n",
        "  target_actor  = build_actor(state_shape, name='TargetActor')\n",
        "  target_critic = build_critic(state_shape, name='TargetCritic')\n",
        "\n",
        "  # Copy parameters from action and critic\n",
        "  target_actor.set_weights(actor.get_weights())\n",
        "  target_critic.set_weights(critic.get_weights())\n",
        "  return target_actor, target_critic"
      ],
      "metadata": {
        "id": "Ps2RIWFaocrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the images (state)\n",
        "def preprocess_state(img, greyscale=False):\n",
        "      img = img.copy()\n",
        "      # Remove numbers and enlarge speed bar\n",
        "      for i in range(88, 93+1):\n",
        "          img[i, 0:12, :] = img[i, 12, :]\n",
        "\n",
        "      # Unify grass color\n",
        "      r1, g1, b1 = (102, 229, 102)\n",
        "      r2, g2, b2 = (102, 204, 102)\n",
        "\n",
        "      red, green, blue = img[:,:,0], img[:,:,1], img[:,:,2]\n",
        "      mask = (red == r1) & (green == g1) & (blue == b1)\n",
        "      img[:,:,:3][mask] = [r2, g2, b2]\n",
        "\n",
        "      # Make car black\n",
        "      car_color = 68.0\n",
        "      car_area = img[67:77, 42:53]\n",
        "      car_area[car_area == car_color] = 0\n",
        "\n",
        "      # Scale from 0 to 1\n",
        "      img = img / img.max()\n",
        "\n",
        "      # Unify track color\n",
        "      img[(img > 0.411) & (img < 0.412)] = 0.4\n",
        "      img[(img > 0.419) & (img < 0.420)] = 0.4\n",
        "\n",
        "      # Change color of kerbs\n",
        "      game_screen = img[0:83, :]\n",
        "      game_screen[game_screen == 1] = 0.80\n",
        "      return img"
      ],
      "metadata": {
        "id": "yVzWbutLnC-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start the training"
      ],
      "metadata": {
        "id": "WjUswJIEodHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize hyper parameters, memory buffer and noise generation"
      ],
      "metadata": {
        "id": "l4_l2SLarkBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MemoriesRecorder:\n",
        "    def __init__(self, memory_capacity=50000):\n",
        "        self.memory_capacity = memory_capacity\n",
        "\n",
        "        # Memory will be initialized when first time used\n",
        "        self.state_db     = None\n",
        "        self.action_db    = None\n",
        "        self.reward_db    = None\n",
        "        self.new_state_db = None\n",
        "\n",
        "        self.writes_num = 0\n",
        "\n",
        "    def init_memory(self, state_shape, action_shape):\n",
        "        state_shape  = prepend_tuple(self.memory_capacity, state_shape)\n",
        "        action_shape = prepend_tuple(self.memory_capacity, action_shape)\n",
        "\n",
        "        self.state_db     = np.zeros(state_shape, np.float32)\n",
        "        self.action_db    = np.zeros(action_shape, np.float32)\n",
        "        self.reward_db    = np.zeros((self.memory_capacity, 1), np.float32)\n",
        "        self.new_state_db = np.zeros(state_shape, np.float32)\n",
        "\n",
        "    def write(self, state, action, reward, new_state):\n",
        "        if self.state_db is None:\n",
        "            self.init_memory(state.shape, action.shape)\n",
        "\n",
        "        # Write indexes\n",
        "        memory_index = self.writes_num     % self.memory_capacity\n",
        "        next_index   = (self.writes_num + 1) % self.memory_capacity\n",
        "\n",
        "        # Save next state to the same array with a next index\n",
        "        self.state_db[memory_index] = state\n",
        "        self.action_db[memory_index]    = action\n",
        "        self.reward_db[memory_index]    = reward\n",
        "        self.new_state_db[memory_index] = new_state\n",
        "\n",
        "        self.writes_num += 1\n",
        "\n",
        "    def sample(self, batch_size=64):\n",
        "        indexes_range = min(self.memory_capacity, self.writes_num)\n",
        "        sampled_indexes = np.random.choice(indexes_range, batch_size)\n",
        "\n",
        "        return (self.state_db[sampled_indexes],\n",
        "                self.action_db[sampled_indexes],\n",
        "                self.reward_db[sampled_indexes],\n",
        "                self.new_state_db[sampled_indexes])\n",
        "\n",
        "# Inserts a new element to the beginning of a tuple.\n",
        "\n",
        "def prepend_tuple(new_dim, some_shape):\n",
        "    some_shape_list = list(some_shape)\n",
        "    some_shape_list.insert(0, new_dim)\n",
        "    return tuple(some_shape_list)"
      ],
      "metadata": {
        "id": "YwYMaVGDrF44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We use the same noise function as the one explained in lecture for ddpg_mountaincar_comt\n",
        "class OUActionNoise:\n",
        "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
        "        self.theta = theta\n",
        "        self.mean = mean\n",
        "        self.std_dev = std_deviation\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
        "        x = (\n",
        "            self.x_prev\n",
        "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
        "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "        )\n",
        "        # Store x into x_prev\n",
        "        # Makes next noise dependent on current one\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        if self.x_initial is not None:\n",
        "            self.x_prev = self.x_initial\n",
        "        else:\n",
        "            self.x_prev = np.zeros_like(self.mean)"
      ],
      "metadata": {
        "id": "WX3e9D4uljvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Training Hyperparameters\n",
        "gamma = 0.99\n",
        "actor_lr = 0.00001\n",
        "critic_lr = 0.002\n",
        "tau = 0.005\n",
        "memory_capacity = 60000\n",
        "n_episodes = 50000\n",
        "std_dev = 0.2\n",
        "noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n"
      ],
      "metadata": {
        "id": "1GgOrjzEjXKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start training loop"
      ],
      "metadata": {
        "id": "UV4cWMNfrpiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop of episodes\n",
        "for ie in range(n_episodes):\n",
        "    state = env.reset()\n",
        "    solution.reset()\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    no_reward_counter = 0\n",
        "\n",
        "    # One-step-loop\n",
        "    while not done:\n",
        "        if preview:\n",
        "            env.render()\n",
        "\n",
        "        action, train_action = solution.get_action(state)\n",
        "\n",
        "        # This will make steering much easier\n",
        "        action /= 4\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Models action output has a different shape for this problem\n",
        "        solution.learn(state, train_action, reward, new_state)\n",
        "        state = new_state\n",
        "        episode_reward += reward\n",
        "\n",
        "        if reward < 0:\n",
        "            no_reward_counter += 1\n",
        "            if no_reward_counter > 200:\n",
        "                break\n",
        "        else:\n",
        "            no_reward_counter = 0\n",
        "\n",
        "    all_episode_reward.append(episode_reward)\n",
        "    average_result = np.array(all_episode_reward[-10:]).mean()\n",
        "    print('Last result:', episode_reward, 'Average results:', average_result)\n",
        "\n",
        "    if episode_reward > best_result:\n",
        "        print('Saving best solution')\n",
        "        solution.save_solution()\n",
        "        best_result = episode_reward"
      ],
      "metadata": {
        "id": "eROt5ulip3ne"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}