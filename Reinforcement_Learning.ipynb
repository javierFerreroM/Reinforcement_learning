{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvL5esIEr5_0"
   },
   "source": [
    "# Car racing problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNFlKhXZGgGB"
   },
   "source": [
    "## Bibliografy\n",
    "https://github.com/andywu0913/OpenAI-GYM-CarRacing-DQN;\n",
    "https://github.com/wpiszlogin/driver_critic/blob/main/base_solution.py; \n",
    "https://www.gymlibrary.ml/environments/box2d/car_racing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlapvPz4r_Zx"
   },
   "source": [
    "## Initialize and install the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "al1yhBkFPhMl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-02 13:24:57.983596: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-02 13:24:57.983631: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from docutils.nodes import topic\n",
    "#from tools import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOL1xkH5Qqdp"
   },
   "source": [
    "## Description\n",
    "The easiest control task to learn from pixels - a top-down racing environment. The generated track is random every episode.\n",
    "\n",
    "Some indicators are shown at the bottom of the window along with the state RGB buffer. From left to right: true speed, four ABS sensors, steering wheel position, and gyroscope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fjkd8e9M6fdH",
    "outputId": "03d8cbe0-abb6-43a3-d20b-0cf81dc31656"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  96\n",
      "Size of Action Space ->  3\n",
      "Actions are ->  Box([-1.  0.  0.], 1.0, (3,), float32)\n",
      "Max Value of Action ->  1.0\n",
      "Min Value of Action ->  -1.0\n",
      "Max Value of Observation ->  255\n",
      "Min Value of Observation ->  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:98: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n"
     ]
    }
   ],
   "source": [
    "# We use version 0 as the original version\n",
    "# This cell describes the characteristics of the problem\n",
    "problem = \"CarRacing-v1\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "action_space = env.action_space\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "print(\"Actions are ->  {}\".format(action_space))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "observation_upper_bound = env.observation_space.high[0][0][0]\n",
    "observation_lower_bound = env.observation_space.low[0][0][0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))\n",
    "\n",
    "print(\"Max Value of Observation ->  {}\".format(observation_upper_bound))\n",
    "print(\"Min Value of Observation ->  {}\".format(observation_lower_bound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhT0Nc12TiLu"
   },
   "source": [
    "##Action space\n",
    "As observed, there are three actions:\n",
    "\n",
    "*   Turn (-1 left and +1 is right)\n",
    "*   Accelerate\n",
    "*   Breaking\n",
    "\n",
    "##Observation space\n",
    "The observation space (the image) is 96x96 pixels\n",
    "\n",
    "\n",
    "##Rewards\n",
    "The reward is -0.1 every frme and +1000/N for every track visited, where N is the total number of tiles visited in the track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points.\n",
    "\n",
    "If the car goes outside the playfield, it receives -100 reward and dies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzuR-Dr3dox9"
   },
   "source": [
    "##Actor-critic and policy definition\n",
    "Actor-critic learning is a reinforcement-learning technique in which you simultaneously learn a policy function and a value function. The policy function tells you how to make decisions, and the value function helps improve the training process for the value function.\n",
    "\n",
    "The actor takes as input the state and outputs the best action. It essentially controls how the agent behaves by learning the optimal policy (policy-based). The critic, on the other hand, evaluates the action by computing the value function (value based). Those two models participate in a game where they both get better in their own role as the time passes. The result is that the overall architecture will learn to play the game more efficiently than the two methods separately.\n",
    "\n",
    "In the following two boxes we define the actor and the critic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkzPAtIksk86"
   },
   "source": [
    "### Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "loQChPcqYLDE"
   },
   "outputs": [],
   "source": [
    "# We build here the behaviour of the actor\n",
    "def build_actor(state_shape, name=\"Actor\"):\n",
    "      inputs = layers.Input(shape=state_shape)\n",
    "      x = inputs\n",
    "      x = layers.Conv2D(16, kernel_size=(5, 5), strides=(4, 4), padding='valid', use_bias=False, activation=\"relu\")(x)\n",
    "      x = layers.Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='valid', use_bias=False, activation=\"relu\")(x)\n",
    "      x = layers.Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='valid', use_bias=False, activation=\"relu\")(x)\n",
    "\n",
    "      x = layers.Flatten()(x)\n",
    "      x = layers.Dense(64, activation='relu')(x)\n",
    "      last_init = tf.random_uniform_initializer(minval=-0.005, maxval=0.005)\n",
    "      y = layers.Dense(num_actions, activation='tanh')(x)\n",
    "\n",
    "      model = Model(inputs=inputs, outputs=y, name=name)\n",
    "      model.summary()\n",
    "      return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8znxfe2Sdhxd"
   },
   "source": [
    "###Critic\n",
    "The value function is the critic: it tracks whether the agent is ahead or behind in the course of the game. That feedback guides the training process, in the same way that a game review can guide your own study.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qBZgj3f4dJwd"
   },
   "outputs": [],
   "source": [
    "# We build here the behaviour of the Critic.\n",
    "def build_critic(state_shape, name=\"Critic\"):\n",
    "    state_inputs = layers.Input(shape=state_shape)\n",
    "    x = state_inputs\n",
    "    x = layers.Conv2D(16, kernel_size=(5, 5), strides=(4, 4), padding='valid', use_bias=False, activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='valid', use_bias=False, activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='valid', use_bias=False, activation=\"relu\")(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    action_inputs = layers.Input(shape=(num_actions,))\n",
    "    x = layers.concatenate([x, action_inputs])\n",
    "\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    y = layers.Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[state_inputs, action_inputs], outputs=y, name=name)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgUCoG8pfVQ2"
   },
   "source": [
    "### Policy\n",
    "Now we define the policy. The policy is a mapping that selects actions based on the observations from the environment. Typically, the policy is a function approximator with tunable parameters, such as a deep neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nQF_RPr_iLaN"
   },
   "outputs": [],
   "source": [
    "# Policy will allow us to get the actions during training\n",
    "def policy(state, params):\n",
    "        prep_state = preprocess_state(state)\n",
    "        if params['actor'] is None:\n",
    "            init_networks(prep_state.shape, params)\n",
    "\n",
    "        # Get result from a network\n",
    "        tensor_state = tf.expand_dims(tf.convert_to_tensor(prep_state), 0)\n",
    "        actor_output = params['actor'](tensor_state).numpy()\n",
    "\n",
    "        # Add noise\n",
    "        actor_output = actor_output[0] + params['noise']()\n",
    "        env_action = actor_output\n",
    "\n",
    "        # Clip min-max\n",
    "        env_action = np.clip(np.array(env_action), a_min= action_space.low, a_max= action_space.high)\n",
    "        return env_action, actor_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ps2RIWFaocrM"
   },
   "outputs": [],
   "source": [
    "# We initialize the networks for the first step\n",
    "def init_networks(state_shape, params):\n",
    "    # Networks initialization\n",
    "    params['actor']  = build_actor(state_shape)\n",
    "    params['critic'] = build_critic(state_shape)\n",
    "    \n",
    "    # Build target networks in the same way\n",
    "    params['target_actor']  = build_actor(state_shape, name='TargetActor')\n",
    "    params['target_critic'] = build_critic(state_shape, name='TargetCritic')\n",
    "    \n",
    "    # Copy parameters from action and critic\n",
    "    params['target_actor'].set_weights(params['actor'].get_weights())\n",
    "    params['target_critic'].set_weights(params['critic'].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yVzWbutLnC-8"
   },
   "outputs": [],
   "source": [
    "# Preprocess the images (state)\n",
    "def preprocess_state(img, greyscale=False):\n",
    "      img = img.copy()\n",
    "      # Remove numbers and enlarge speed bar\n",
    "      for i in range(88, 93+1):\n",
    "          img[i, 0:12, :] = img[i, 12, :]\n",
    "\n",
    "      # Unify grass color\n",
    "      r1, g1, b1 = (102, 229, 102)\n",
    "      r2, g2, b2 = (102, 204, 102)\n",
    "\n",
    "      red, green, blue = img[:,:,0], img[:,:,1], img[:,:,2]\n",
    "      mask = (red == r1) & (green == g1) & (blue == b1)\n",
    "      img[:,:,:3][mask] = [r2, g2, b2]\n",
    "\n",
    "      # Make car black\n",
    "      car_color = 68.0\n",
    "      car_area = img[67:77, 42:53]\n",
    "      car_area[car_area == car_color] = 0\n",
    "\n",
    "      # Scale from 0 to 1\n",
    "      img = img / img.max()\n",
    "\n",
    "      # Unify track color\n",
    "      img[(img > 0.411) & (img < 0.412)] = 0.4\n",
    "      img[(img > 0.419) & (img < 0.420)] = 0.4\n",
    "\n",
    "      # Change color of kerbs\n",
    "      game_screen = img[0:83, :]\n",
    "      game_screen[game_screen == 1] = 0.80\n",
    "      return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjUswJIEodHZ"
   },
   "source": [
    "# Start the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4_l2SLarkBR"
   },
   "source": [
    "## Initialize hyper parameters, memory buffer and noise generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YwYMaVGDrF44"
   },
   "outputs": [],
   "source": [
    "class MemoriesRecorder:\n",
    "    def __init__(self, memory_capacity=50000):\n",
    "        self.memory_capacity = memory_capacity\n",
    "\n",
    "        # Memory will be initialized when first time used\n",
    "        self.state_db     = None\n",
    "        self.action_db    = None\n",
    "        self.reward_db    = None\n",
    "        self.new_state_db = None\n",
    "\n",
    "        self.writes_num = 0\n",
    "\n",
    "    def init_memory(self, state_shape, action_shape):\n",
    "        state_shape  = prepend_tuple(self.memory_capacity, state_shape)\n",
    "        action_shape = prepend_tuple(self.memory_capacity, action_shape)\n",
    "\n",
    "        self.state_db     = np.zeros(state_shape, np.float32)\n",
    "        self.action_db    = np.zeros(action_shape, np.float32)\n",
    "        self.reward_db    = np.zeros((self.memory_capacity, 1), np.float32)\n",
    "        self.new_state_db = np.zeros(state_shape, np.float32)\n",
    "\n",
    "    def write(self, state, action, reward, new_state):\n",
    "        if self.state_db is None:\n",
    "            self.init_memory(state.shape, action.shape)\n",
    "\n",
    "        # Write indexes\n",
    "        memory_index = self.writes_num     % self.memory_capacity\n",
    "        next_index   = (self.writes_num + 1) % self.memory_capacity\n",
    "\n",
    "        # Save next state to the same array with a next index\n",
    "        self.state_db[memory_index] = state\n",
    "        self.action_db[memory_index]    = action\n",
    "        self.reward_db[memory_index]    = reward\n",
    "        self.new_state_db[memory_index] = new_state\n",
    "\n",
    "        self.writes_num += 1\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        indexes_range = min(self.memory_capacity, self.writes_num)\n",
    "        sampled_indexes = np.random.choice(indexes_range, batch_size)\n",
    "\n",
    "        return (self.state_db[sampled_indexes],\n",
    "                self.action_db[sampled_indexes],\n",
    "                self.reward_db[sampled_indexes],\n",
    "                self.new_state_db[sampled_indexes])\n",
    "\n",
    "# Inserts a new element to the beginning of a tuple.\n",
    "\n",
    "def prepend_tuple(new_dim, some_shape):\n",
    "    some_shape_list = list(some_shape)\n",
    "    some_shape_list.insert(0, new_dim)\n",
    "    return tuple(some_shape_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WX3e9D4uljvi"
   },
   "outputs": [],
   "source": [
    "#We use the same noise function as the one explained in lecture for ddpg_mountaincar_comt\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(state, train_action, reward, new_state, params):\n",
    "        # Store transition in R\n",
    "        prep_state     = preprocess_state(state)\n",
    "        prep_new_state = preprocess_state(new_state)\n",
    "        params['r_buffer'].write(prep_state, train_action, reward, prep_new_state)\n",
    "\n",
    "        # Sample mini-batch from R\n",
    "        state_batch, action_batch, reward_batch, new_state_batch  = params['r_buffer'].sample()\n",
    "\n",
    "        state_batch     = tf.convert_to_tensor(state_batch)\n",
    "        action_batch    = tf.convert_to_tensor(action_batch)\n",
    "        reward_batch    = tf.convert_to_tensor(reward_batch)\n",
    "        reward_batch    = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        new_state_batch = tf.convert_to_tensor(new_state_batch)\n",
    "\n",
    "        update_actor_critic(state_batch, action_batch, reward_batch, new_state_batch, params)\n",
    "\n",
    "        # Update target networks\n",
    "        update_target_network(params['target_actor'].variables, params['actor'].variables)\n",
    "        update_target_network(params['target_critic'].variables, params['critic'].variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_actor_critic(state, action, reward, new_state, params):\n",
    "    # Update critic\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Calc y\n",
    "        new_action = params['target_actor'](new_state, training=True)\n",
    "        y = reward + params['gamma'] * params['target_critic']([new_state, new_action], training=True)\n",
    "\n",
    "        critic_loss = tf.math.reduce_mean(tf.square(y - params['critic']([state, action], training=True)))\n",
    "\n",
    "    critic_gradients = tape.gradient(critic_loss, params['critic'].trainable_variables)\n",
    "    #critic_opt = params['critic_opt']\n",
    "    params['critic_opt'].apply_gradients(zip(critic_gradients, params['critic'].trainable_variables)) \n",
    "\n",
    "    # Update actor\n",
    "    with tf.GradientTape() as tape:\n",
    "        critic_out = params['critic']([state, params['actor'](state, training=True)], training=True)\n",
    "        actor_loss = -tf.math.reduce_mean(critic_out)  # Need to maximize\n",
    "\n",
    "    actor_gradients = tape.gradient(actor_loss, params['actor'].trainable_variables)\n",
    "    params['actor_opt'].apply_gradients(zip(actor_gradients, params['actor'].trainable_variables))\n",
    "\n",
    "@tf.function\n",
    "def update_target_network(target_weights, new_weights):\n",
    "    for t, n in zip(target_weights, new_weights):\n",
    "        t.assign((1 - params['tau']) * t + params['tau'] * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1GgOrjzEjXKG"
   },
   "outputs": [],
   "source": [
    " # Training Hyperparameters\n",
    "params = {\n",
    "    'gamma': 0.99,\n",
    "    'actor_lr': 0.00001,\n",
    "    'critic_lr': 0.002,\n",
    "    'actor_opt': None,\n",
    "    'critic_opt': None,\n",
    "    'actor': None,\n",
    "    'target_actor': None,\n",
    "    'target_critic': None,\n",
    "    'critic': None,\n",
    "    'tau': 0.005,\n",
    "    'memory_capacity': 60000,\n",
    "    'n_episodes': 50000,\n",
    "    'std_dev': 0.2,\n",
    "    'noise': None,\n",
    "    'r_buffer': None,\n",
    "    'all_episode_reward': [],\n",
    "    'preview': False,\n",
    "    'best_result': 0,\n",
    "}\n",
    "\n",
    "params['actor_opt'] = Adam(params['actor_lr'])\n",
    "params['critic_opt'] = Adam(params['critic_lr'])\n",
    "params['r_buffer'] = MemoriesRecorder(params['memory_capacity'])\n",
    "params['noise'] = OUActionNoise(mean=np.zeros(1), std_deviation=float(params['std_dev']) * np.ones(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UV4cWMNfrpiB"
   },
   "source": [
    "## Start training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "eROt5ulip3ne",
    "outputId": "de953f23-e3d3-45fa-ca6e-649a995ee6d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-02 13:25:00.389595: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-02 13:25:00.389685: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-02 13:25:00.389714: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-46-80.eu-west-3.compute.internal): /proc/driver/nvidia/version does not exist\n",
      "2022-07-02 13:25:00.390006: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Actor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 23, 23, 16)        1200      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 7, 7, 32)          4608      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 2, 2, 32)          9216      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,475\n",
      "Trainable params: 23,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Critic\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 96, 96, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 23, 23, 16)   1200        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 7, 7, 32)     4608        ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 2, 2, 32)     9216        ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 128)          0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 131)          0           ['flatten_1[0][0]',              \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           8448        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 32)           2080        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            33          ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 25,585\n",
      "Trainable params: 25,585\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"TargetActor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 23, 23, 16)        1200      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 7, 7, 32)          4608      \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 2, 2, 32)          9216      \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,475\n",
      "Trainable params: 23,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"TargetCritic\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 96, 96, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 23, 23, 16)   1200        ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 7, 7, 32)     4608        ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 2, 2, 32)     9216        ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 128)          0           ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 131)          0           ['flatten_3[0][0]',              \n",
      "                                                                  'input_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 64)           8448        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 32)           2080        ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            33          ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 25,585\n",
      "Trainable params: 25,585\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Last result: -18.030111524163537 Average results: -18.030111524163537\n",
      "Last result: -12.167871485943737 Average results: -15.098991505053636\n",
      "Last result: 2.880281690140537 Average results: -9.105900439988913\n",
      "Saving best solution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last result: -9.253424657534156 Average results: -9.142781494375223\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26886/2016148691.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Models action output has a different shape for this problem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#updated_actor, updated_critic, updated_target_network_actor, updated_target_network_critic =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_26886/3079341813.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(state, train_action, reward, new_state, params)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mnew_state_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mupdate_actor_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Update target networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_26886/1168022785.py\u001b[0m in \u001b[0;36mupdate_actor_critic\u001b[0;34m(state, action, reward, new_state, params)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'critic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcritic_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'critic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m#critic_opt = params['critic_opt']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'critic_opt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'critic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    588\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m           data_format=data_format),\n\u001b[0m\u001b[1;32m    591\u001b[0m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[1;32m    592\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1244\u001b[0;31m         data_format, \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1245\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop of episodes\n",
    "for ie in range(params['n_episodes']):\n",
    "    state = env.reset()\n",
    "    params['noise'].reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    no_reward_counter = 0\n",
    "\n",
    "    # One-step-loop\n",
    "    while not done:\n",
    "        action, train_action = policy(state, params)\n",
    "\n",
    "        # This will make steering much easier\n",
    "        action /= 4\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Models action output has a different shape for this problem\n",
    "        #updated_actor, updated_critic, updated_target_network_actor, updated_target_network_critic = \n",
    "        learn(state, train_action, reward, new_state, params)\n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if reward < 0:\n",
    "            no_reward_counter += 1\n",
    "            if no_reward_counter > 200:\n",
    "                break\n",
    "        else:\n",
    "            no_reward_counter = 0\n",
    "\n",
    "    params['all_episode_reward'].append(episode_reward)\n",
    "    average_result = np.array(params['all_episode_reward'][-10:]).mean()\n",
    "    print('Last result:', episode_reward, 'Average results:', average_result)\n",
    "\n",
    "    if episode_reward > params['best_result']:\n",
    "        print('Saving best solution')\n",
    "        #solution.save_solution()\n",
    "        params['best_result'] = episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMY5prsx1exFlnVmvJe0KXh",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Reinforcement Learning.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
